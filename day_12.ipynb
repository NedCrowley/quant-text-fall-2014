{
 "metadata": {
  "name": "",
  "signature": "sha256:c8f00232337664c2342b61584b00e44b460a21d1ae2f9a81cafb328de9cf37cd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'https://gist.githubusercontent.com/nealcaren/886c85645695d04887d0/raw/fec90e5cc1257f277f97a904a4f1cb77f60b2012/asa_2014_prelim.csv'\n",
      "response = urllib2.urlopen(url)\n",
      "reader = csv.reader(response)\n",
      "presentations = list(reader)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "roundtable = []\n",
      "abstracts  = []\n",
      "\n",
      "for presentation in presentations:\n",
      "    abstract = presentation[-1]\n",
      "    if len(abstract) > 10:\n",
      "        abstracts.append(abstract)\n",
      "        if 'Roundtable' in presentation[3]:\n",
      "            roundtable.append(1)\n",
      "        else:\n",
      "            roundtable.append(0)\n",
      "\n",
      "print len(abstracts)\n",
      "print len(roundtable)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3698\n",
        "3698\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(lowercase     = True ,\n",
      "                             min_df        = .1 ,\n",
      "                             max_df        = .8 ,\n",
      "                             ngram_range   = (1, 1),\n",
      "                             stop_words    = 'english', \n",
      "                             strip_accents = 'unicode')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.fit(abstracts)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 87,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=0.8, max_features=None, min_df=0.1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "        strip_accents='unicode', token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.transform(abstracts)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = MultinomialNB()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(X,roundtable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_hat = clf.predict(X)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X, roundtable)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "0.60519199567333692"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.confusion_matrix(roundtable, y_hat)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "array([[ 535,  954],\n",
        "       [ 506, 1703]])"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What are the most informative features?\n",
      "# http://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
      "\n",
      "def show_most_informative_features(vectorizer, clf, n=20):\n",
      "    feature_names = vectorizer.get_feature_names()\n",
      "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
      "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
      "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
      "        print \"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_most_informative_features(vectorizer, clf, n=15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\t-4.9764\timplications   \t\t-2.7775\tsocial         \n",
        "\t-4.9725\tused           \t\t-3.2860\tpaper          \n",
        "\t-4.9687\tparticular     \t\t-3.3431\tstudy          \n",
        "\t-4.9571\trecent         \t\t-3.3955\tresearch       \n",
        "\t-4.9381\tfocus          \t\t-3.5158\twomen          \n",
        "\t-4.8797\tcase           \t\t-3.5843\thealth         \n",
        "\t-4.8621\tyears          \t\t-3.7281\tdata           \n",
        "\t-4.7786\tinfluence      \t\t-3.8148\twork           \n",
        "\t-4.7690\tways           \t\t-3.9592\tusing          \n",
        "\t-4.7502\tunderstanding  \t\t-4.0069\tstate          \n",
        "\t-4.7471\tdifferences    \t\t-4.0324\teconomic       \n",
        "\t-4.7471\timpact         \t\t-4.0431\tgender         \n",
        "\t-4.7256\tprocess        \t\t-4.0477\tpolitical      \n",
        "\t-4.7256\tsuggest        \t\t-4.0680\tanalysis       \n",
        "\t-4.7165\tunited         \t\t-4.0743\tuse            \n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One problem with this, as we discovered is that it is very easy to overfit your model to the data. These models tend to be really good about telling you about correlates in your dataset, but very bad at telling you anything meaningful and generalizable. \n",
      "\n",
      "One solution to this problem is split the cases into a training set (for fitting the model) and a test set (for evaluating the fit statistics). Models that fit the training set too well tend to perform very badly on the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Split into a training and test, saving 1/3 of the cases for the test\n",
      "\n",
      "train_X, test_X, train_y, test_y = train_test_split(X, roundtable, test_size=0.33)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(train_X,train_y)\n",
      "print clf.score(train_X,train_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.909971740008\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#That's a pretty high fit score, but is it realistic?\n",
      "print clf.score(test_X,test_y)\n",
      "y_pred = clf.predict(test_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.60687960688\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# More fit stats\n",
      "\n",
      "print \"accuracy:\", metrics.accuracy_score(test_y, y_pred)\n",
      "print \"precision:\", metrics.precision_score(test_y, y_pred)\n",
      "print \"recall:\", metrics.recall_score(test_y, y_pred)\n",
      "print \"f1 score:\", metrics.f1_score(test_y, y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy: 0.60687960688\n",
        "precision: 0.667128987517\n",
        "recall: 0.667128987517\n",
        "f1 score: 0.667128987517\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(test_y, y_pred,\n",
      "                                    target_names=['talk', 'roundtable'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "       talk       0.52      0.52      0.52       500\n",
        " roundtable       0.67      0.67      0.67       721\n",
        "\n",
        "avg / total       0.61      0.61      0.61      1221\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another way to test the fit is to split the sample into two equal sized pieces. First train on one half, then test on the other. Then swap it around by estimating the parameters with the second half and testing those on the second half. Your fit score is the average between the two."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X1, X2, y1, y2 = train_test_split(X, roundtable, test_size=0.5)\n",
      "\n",
      "y2_pred = clf.fit(X1, y1).predict(X2)\n",
      "y1_pred = clf.fit(X2, y2).predict(X1)\n",
      "\n",
      "print np.mean([metrics.precision_score(y1, y1_pred),\n",
      "               metrics.precision_score(y2, y2_pred)])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.660767925791\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More generally, this is called cross validation, and scikit-learn can do this for you. In this case, we will split the sample into 10 pieces. We estimate the model 10 times, each time using 90% to estimate the model and the leftout 10% to test the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores =  cross_val_score(clf, X, roundtable, cv=10, scoring='f1')\n",
      "print scores\n",
      "print 'Average f1 score:',np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.70089286  0.6332574   0.65342163  0.67573696  0.640553    0.68444444\n",
        "  0.65898618  0.62773723  0.67256637  0.58741259]\n",
        "Average f1 score: 0.653500865566\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This model is fine, but you might want to go back and try different parameters for either the vectorizer or the classifier (or later, try and compare different classifiers). Again scikit-learn has a built-in solution with `Pipeline`. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, tell it all the parts of your model. Ours is fairly simple. \n",
      "# For the vectorizer, I only included the options that I don't want change.\n",
      "\n",
      "pipeline = Pipeline([('vect', CountVectorizer(lowercase = True, stop_words = 'english',strip_accents = 'unicode')),\n",
      "                    ('clf', MultinomialNB()) ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, I set up a dictionary with all the possible values I want it to test. \n",
      "# In this case, for example, it will look  at three different options for n-gram ranges,\n",
      "# including ngrams (1,1), bigrams(1,2) and trigrams(1,3). \n",
      "# In total, it will estimate 3 x 4 x 3, or 36 different models  \n",
      "\n",
      "parameters = {'vect__max_df': (0.5, 0.75, 1.0), \n",
      "              'vect__min_df': (2, 5, .01, .02), \n",
      "              'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
      "              }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cv=10 because I want it to use 10 fold cross-validation. \n",
      "# n_jobs = -1 uses all the computer processing power it wants \n",
      "# score = 'f1' tells it what fit metric to maximize\n",
      "\n",
      "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, cv=10, scoring='f1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run the search\n",
      "# note that since this includes the vectorize, the X input is the raw text data!!!\n",
      "# This will take a long time and make your computer sweat\n",
      "\n",
      "grid_search.fit(abstracts, roundtable)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "GridSearchCV(cv=10,\n",
        "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), prep...enizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
        "       fit_params={}, iid=True, loss_func=None, n_jobs=-1,\n",
        "       param_grid={'vect__ngram_range': ((1, 1), (1, 2), (1, 3)), 'vect__min_df': (2, 5, 0.01, 0.02), 'vect__max_df': (0.5, 0.75, 1.0)},\n",
        "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring='f1',\n",
        "       verbose=0)"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tvect__max_df: 0.5\n",
        "\tvect__min_df: 2\n",
        "\tvect__ngram_range: (1, 2)\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(lowercase     = True ,\n",
      "                             min_df        =  2 ,\n",
      "                             max_df        = .5 ,\n",
      "                             ngram_range   = (1, 2),\n",
      "                             stop_words    = 'english', \n",
      "                             strip_accents = 'unicode')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.fit_transform(abstracts)\n",
      "scores = cross_val_score(clf, X, roundtable, cv=10, scoring='f1')\n",
      "print scores\n",
      "np.mean(scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.66666667  0.61716937  0.67849224  0.66666667  0.625       0.67114094\n",
        "  0.68649886  0.61835749  0.65617978  0.5990566 ]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "0.64852286087608113"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_most_informative_features(vectorizer, clf, n=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\t-12.7451\t000 employees  \t\t-5.0010\tsocial         \n",
        "\t-12.7451\t000 news       \t\t-5.5095\tpaper          \n",
        "\t-12.7451\t052            \t\t-5.5665\tstudy          \n",
        "\t-12.7451\t100 countries  \t\t-5.6190\tresearch       \n",
        "\t-12.7451\t112 country    \t\t-5.7393\twomen          \n",
        "\t-12.7451\t130            \t\t-5.8078\thealth         \n",
        "\t-12.7451\t137            \t\t-5.9516\tdata           \n",
        "\t-12.7451\t1376           \t\t-6.0382\twork           \n",
        "\t-12.7451\t140            \t\t-6.1826\tusing          \n",
        "\t-12.7451\t141            \t\t-6.2215\tstudents       \n",
        "\t-12.7451\t17 societies   \t\t-6.2304\tstate          \n",
        "\t-12.7451\t18 19          \t\t-6.2559\teconomic       \n",
        "\t-12.7451\t18 countries   \t\t-6.2666\tgender         \n",
        "\t-12.7451\t18 month       \t\t-6.2712\tpolitical      \n",
        "\t-12.7451\t1870s          \t\t-6.2915\tanalysis       \n",
        "\t-12.7451\t1880           \t\t-6.2978\tuse            \n",
        "\t-12.7451\t1922           \t\t-6.3907\tnew            \n",
        "\t-12.7451\t1933           \t\t-6.4047\tfamily         \n",
        "\t-12.7451\t1960 2010      \t\t-6.4065\tschool         \n",
        "\t-12.7451\t1977 2005      \t\t-6.4136\tbased          \n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}